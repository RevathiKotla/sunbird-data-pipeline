kafka {
  input.topic = "sunbirddev.telemetry.raw"
  output.failed.topic = "sunbirddev.telemetry.failed"
  output.primary.route.topic = "sunbirddev.telemetry.sink"
  output.secondary.route.topic = "sunbirddev.telemetry.log"
  output.audit.route.topic = "sunbirddev.telemetry.audit"
  output.duplicate.topic = "sunbirddev.telemetry.duplicate"
  output.metrics.topic = "k8s.telemetry.metrics.flink"
  broker-servers = "192.168.43.149:9092"
  zookeeper = "192.168.43.149:2181"
  # dev-environment
   #broker-servers = "11.2.1.15:9092"
   #zookeeper = "11.2.1.15:2181"
  groupId = "pipeline-preprocessor-group"
}
task {
  parallelism = 2
  checkpointing.interval = 60000
  metrics.window.size = 5000 # 3 min
  restart-strategy.attempts = 1 # retry once
  restart-strategy.delay = 1000 # in milli-seconds
}
telemetry.validation.parallelism = 1
telemetry.router.parallelism = 1
share.events.flattener.parallelism = 1
telemetry.schema.path="schemas/telemetry/3.0"
router.secondary.routes.eid = ["LOG", "ERROR"]
default.channel="org.sunbird"
dedup.producer.included.ids = ["prod.diksha.portal", "prod.sunbird.desktop"]
redis {
  host = 127.0.0.1
  # dev-environment
  # host = 11.2.4.22
  port = 6340
  connection {
    max = 2
    idle.min = 1
    idle.max = 2
    minEvictableIdleTimeSeconds = 120
    timeBetweenEvictionRunsSeconds = 300
  }
  database {
    duplicationstore.id = 12
    key.expiry.seconds = 3600
  }
}